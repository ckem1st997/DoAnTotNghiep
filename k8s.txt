tai file
scp -r ubuntu@192.168.3.96:/data/compose/98/minio_data/* /k_minio/
./keyhunt -m address -t 3 -f tests/btc-list-address.txt

ps aux | grep check-btc


export EDITOR=nano

helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=hopnk.xyz \
  --set replicas=1 \
  --set bootstrapPassword=6HjhRwfdsfds43

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC='--flannel-backend=none --disable-network-policy --disable-kube-proxy' sh -
sudo systemctl daemon-reload
sudo systemctl restart k3s


fix 509 rancher k3s
cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
check
pgrep -f ltc-check


cilium install --version 1.14.9


cilium
cilium config  set vtep.endpoint="10.169.72.236 10.169.72.238"    set vtep.cidr="10.1.1.0/24   10.1.2.0/24"    set vtep.mask="255.255.255.0"    set vtep.mac="82:36:4c:98:2e:56 82:36:4c:98:2e:58"
cilium config set enable-vtep true


version: '3.4'

services:
  kong-migrations:
     image: kong:latest
     container_name: kong-migrations
     restart: on-failure
     environment:
         KONG_DATABASE: postgres
         KONG_PG_HOST: 192.168.3.162
         KONG_PG_USER: postgres
         KONG_PG_PASSWORD: abc@123
         KONG_PG_DATABASE: kong-db
       #  KONG_CASSANDRA_CONTACT_POINTS: kong-database
     entrypoint: sh -c "sleep 100 && kong migrations bootstrap --vv"



find / -name 'btc-wallet.txt' 2>/dev/null | xargs du -h

  GNU nano 6.2                                              mkcert+5-key.pem                                                        
-----BEGIN PRIVATE KEY-----
MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCsw+lMT66QKbKw
ggRtpa+afuH7RstCLrXbGbcAIOaI2hfGN6ysVvVNoaA7bbR0Bf/hCGFg2k4JRjMa
63lc1Y3Pr6U6AFgyaZ65aKCuR8u/K6uSPCDNO6v3z398uOXEB0yEtjqN3AiPLQcD
685wdJ6yafqOv4HeFreUGWCR1NFqjpK916jKC+2yj0GD4JryGQVt2HheF+TSRmln
IaTQiaJ5G6KbLD/y9CZyAE1802KJd2pGWlHJLN9+M+th0857rU+hAZcGHQ13OgLp
eTrFAu+djVDY3maXRa4Gk88UqZ3hdqcfyZTXjJo6vlJS1Jw7I2MzzOWqQxC5+Ti1
OJdmqgwVAgMBAAECggEBAJgbwTtb6ICrBdWcE5QKTarDzekzcl/pylmGsXWegfq5
sjbl69Sslztrcyz/sOJ6KciSXVItwokt0m8AJZFTwJC7v8hqZzyze3kYomfvdHbz
VxLgd6zavdToTwGbtUg7eOZbnwApWyvb5BJSuiaOtu60mlK2T/wemjD9iuW6r0qV
nDGNekQbxgiuGtV9Vzcd1YPuLjK671d+GAqs5dZrecRR2e304fClPfzRbuu4bUoO
DLhGxRoiaz08+lV32diUAmXMK/KC2Fqf45hHqmF06VS5XSb82DiPjS1ad46X+qT2
icRDrUnie/Xl16SGzjaOuxZZvl5wDmLydgi22WKEVCECgYEAzQVARQiTWUt+rz7X
4I91CAdkbGhBLLsvH0xMs6zJjxD5iXfYoAfZuOLBFw71N65ruOHwWhH+ofzkjrfn
p6Ykd/iWArYEnFlsbpL8pSGKWryOp1hjBJs8j41mznKVnkObmDFYflPQPdRc9MCy
0B2VLOtQuITyRXhQUbNQjCZu1lkCgYEA17ltSnyAvPOiqf1oZa2w5K9BghLXM88j
v+vlh0L8ym3ReVkRnc8BSXnUS2n2R9e5vWfAqIxR9Cjhwq/0nGvswHR6SJkg2rhj
gJOAwam9HZwRbydbtoFwMn3zgzeays2FvO8QuuG0kiKWxq/lRybEKvuLkF732lmZ
xyI4G4lJZB0CgYAwNeLmGN0la1ZP+VQkTzey+qIHyABDd4TeND/5JFGFXfV/+Pms
sAvnqRLEzFSnklFseVzeoUqiZuiMEvzbdHsw5vjHjGvLJFAWNGRKpHd3XM2i1mr6
Y1K7YwHg3BsIy1ckU74+fpECaRSuzT1/KOWyWWdy7HhYE7G3dJYlTshTiQKBgHHZ
F1e3K04NGmyyIxV+PBwEbgMSpGZgTFKuUT04WicPG/z5x7VMGTbIyARg0KwLqqfO
CLwrCdlcdgEEUYj34EOVdS+5OMCVjVW/ArWyOe9VfVH3KIzB9Q+ArzIx90xOeOVE
GcU29tAk1wPWsp8BU4gHpxkzNYUTwnd4lLnbk1L1AoGBAMajiFfmPXXpNE8QrRTz
gVtusnH7ixxNITphcZGffXaZteUcdC6s3UVqy1/QYW6XlqjQK+uGXqGNzzlDCBjo
qgiK7akKmCjSWT0gKZfuIbxdydrkrGSptWxV/xf75T/F12QTGst7Q81WN6z6Ke6k
uu9k7HMWWBfrRega8lc6IbXg
-----END PRIVATE KEY-----








-----BEGIN CERTIFICATE-----
MIIEQjCCAqqgAwIBAgIRAIc5/Q1YrIAVHBLtOGn6CbQwDQYJKoZIhvcNAQELBQAw
XTEeMBwGA1UEChMVbWtjZXJ0IGRldmVsb3BtZW50IENBMRkwFwYDVQQLDBByb290
QGhhYm9yLWltYWdlMSAwHgYDVQQDDBdta2NlcnQgcm9vdEBoYWJvci1pbWFnZTAe
Fw0yNDAzMjEwMzU5MjNaFw0yNjA2MjEwMzU5MjNaMEQxJzAlBgNVBAoTHm1rY2Vy
dCBkZXZlbG9wbWVudCBjZXJ0aWZpY2F0ZTEZMBcGA1UECwwQcm9vdEBoYWJvci1p
bWFnZTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKzD6UxPrpApsrCC
BG2lr5p+4ftGy0IutdsZtwAg5ojaF8Y3rKxW9U2hoDtttHQF/+EIYWDaTglGMxrr
eVzVjc+vpToAWDJpnrlooK5Hy78rq5I8IM07q/fPf3y45cQHTIS2Oo3cCI8tBwPr
znB0nrJp+o6/gd4Wt5QZYJHU0WqOkr3XqMoL7bKPQYPgmvIZBW3YeF4X5NJGaWch
pNCJonkbopssP/L0JnIATXzTYol3akZaUcks334z62HTznutT6EBlwYdDXc6Aul5
OsUC752NUNjeZpdFrgaTzxSpneF2px/JlNeMmjq+UlLUnDsjYzPM5apDELn5OLU4
l2aqDBUCAwEAAaOBlTCBkjAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYB
BQUHAwEwHwYDVR0jBBgwFoAUoW3H6bBMCco+bNZlGmVQdqU5ag8wSgYDVR0RBEMw
QYIGbWtjZXJ0gghoYWNvbS52boIKKi5oYWNvbS52boIJbG9jYWxob3N0hwR/AAAB
hxAAAAAAAAAAAAAAAAAAAAABMA0GCSqGSIb3DQEBCwUAA4IBgQBeWa7HpvQCEk1O
1q7J81pU1p4nVp3TVclqraFsL53LbKUMkf8Ln8LHI7OCSWMZgxbUjjTAN15LXTrN
k11Hk8prKF+/R+Xm0WOFop/xILdAdl1xBiBDSLo3qVNQLQcNhgSa2AzflWSOawVD
xiBP/Wk3Ua0rCxtNjMxvYoKJ856LhHOXAyFK04fMlxoqfqfQyDWO1UNw5DTPW8zW
QOwzvaGvf6x5c+z8jLZqrQM0wUB3Y+tN+oW1PeoR/uUfIynNf1ZqAepo4gvi3sVT
un+1Nc6Z0fQ3Oo4+gTeme6wkYWB6dclQ0yeWQAG0sc9Xy/bghq4IGroeBsBF/+RY
DbvBTIA5w4wxdU2AQGrjXc7HbrythBGpmCHjc6kYvP1avIYLipMarVvfB6/+Mju0
eht2EyXXw5PD20bfKLWPw+BKsu+krBf+gVtQp0VE5l/NBPcJFBzH3i8iUcUG/m07
uVudf/YFk5CKzWPSEC+94a69kXzT/AG1Rvoetk3bwa5H01PU9lQ=
-----END CERTIFICATE-----




















k3s
curl -sfL https://get.k3s.io | K3S_KUBE_VERSION=v1.27.7 sh -
0R4O8NjDFGub3t1T
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDiFDtdaVJkGTMToRGfKW0m07eFznEc3DZGM71Q2sw>

ssh-keygen -t rsa -b 4096 -C "admin@gamil.com"
IHD2gesPm6Stu5RsTp87M74eLxbVySKW8Wl1YhEVxG/MOU= root@habor-image

curl --insecure -sfL https://192.168.3.96:8443/v3/import/j52g286885xcvq7t4tb8tkctqd9k7kn9zf8qwvvl7x598jp56pzwcm_c-m-qshjp6dz.yaml | kubectl apply -f -
istio
- thêm vào pod
app=web-manager
version=v1


thư viện search động
https://alirezanet.github.io/Gridify/guide/extensions/elasticsearch

dương lượng data docker

du -h --max-depth=1
du -h --max-depth=1 | sort -h

version: "3.6"
services:
  graphql-engine:
    image: hasura/graphql-engine:v2.37.0
    ports:
      - "8686:8080"
    restart: unless-stopped
    environment:
      ## postgres database to store Hasura metadata
      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://postgres:abc%40123@192.168.3.162:5432/postgres
      ## this env var can be used to add the above ms sql server database to Hasura as a data source. this can be removed/updated based on your needs
      MSSQL_DATABASE_URL: Driver={ODBC Driver 18 for SQL Server};Server=192.168.3.162,1433;Database=MasterData;Uid=sa;Pwd=abc@1234567890;Encrypt=optional

      ## enable the console served by server
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true" # set to "false" to disable console
      ## enable debugging mode. It is recommended to disable this in production
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      ## uncomment next line to run console offline (i.e load console assets from server instead of CDN)
      # HASURA_GRAPHQL_CONSOLE_ASSETS_DIR: /srv/console-assets
      ## uncomment next line to set an admin secret
      # HASURA_GRAPHQL_ADMIN_SECRET: myadminsecretkey




fix không có quyền git runner

sudo usermod -aG docker gitlab-runner
sudo usermod -aG docker gitlab-runner
sudo chmod 666 /var/run/docker.sock
 hoặc bổ sung sau khi wor trên không được
stages:
  - build

build:
  script:
    - sudo docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY_IMAGE
    - sudo docker build -t $CI_REGISTRY_IMAGE .
    - sudo docker push $CI_REGISTRY_IMAGE
  only:
    - develop



457

To completely uninstall Docker:

Step 1

dpkg -l | grep -i docker
To identify what installed package you have:

Step 2

sudo apt-get purge -y docker-engine docker docker.io docker-ce docker-ce-cli docker-compose-plugin
sudo apt-get autoremove -y --purge docker-engine docker docker.io docker-ce docker-compose-plugin
The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following commands:

sudo rm -rf /var/lib/docker /etc/docker
sudo rm /etc/apparmor.d/docker
sudo groupdel docker
sudo rm -rf /var/run/docker.sock
You have removed Docker from the system completely.
sudo snap remove docker

git submodule add  ../../../share/hacom.sharedrefs.git Sharedrefs


-- DROP SCHEMA dbo;

CREATE SCHEMA dbo;
-- MasterData.dbo.ListApp definition

-- Drop table

-- DROP TABLE MasterData.dbo.ListApp;

CREATE TABLE MasterData.dbo.ListApp (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	Name nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	OnDelete bit NULL,
	Description nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	IsAPI bit NULL,
	InActive bit NULL,
	SoftShow int NULL,
	ParentId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	CONSTRAINT PK_ListApp PRIMARY KEY (Id)
);


-- MasterData.dbo.ListAuthozire definition

-- Drop table

-- DROP TABLE MasterData.dbo.ListAuthozire;

CREATE TABLE MasterData.dbo.ListAuthozire (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	Name nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	OnDelete bit NULL,
	Description nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	InActive bit NULL,
	SoftShow int NULL,
	ParentId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	CONSTRAINT PK_ListAuthozire PRIMARY KEY (Id)
);


-- MasterData.dbo.ListAuthozireByListRole definition

-- Drop table

-- DROP TABLE MasterData.dbo.ListAuthozireByListRole;

CREATE TABLE MasterData.dbo.ListAuthozireByListRole (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	AppId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	ListRoleId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	OnDelete bit NULL,
	AuthozireId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	CONSTRAINT PK__ListAuth__3214EC07F6C9D6BE PRIMARY KEY (Id)
);


-- MasterData.dbo.ListAuthozireRoleByUser definition

-- Drop table

-- DROP TABLE MasterData.dbo.ListAuthozireRoleByUser;

CREATE TABLE MasterData.dbo.ListAuthozireRoleByUser (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	ListAuthozireId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	OnDelete bit NULL,
	UserId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	CONSTRAINT PK_ListAuthozireRoleByUser PRIMARY KEY (Id)
);


-- MasterData.dbo.ListRole definition

-- Drop table

-- DROP TABLE MasterData.dbo.ListRole;

CREATE TABLE MasterData.dbo.ListRole (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	AppId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	Name nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	OnDelete bit NULL,
	Description nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	InActive bit NULL,
	[Key] nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	ParentId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	SoftShow int NULL,
	IsAPI bit NULL
);


-- MasterData.dbo.ListRoleByUser definition

-- Drop table

-- DROP TABLE MasterData.dbo.ListRoleByUser;

CREATE TABLE MasterData.dbo.ListRoleByUser (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	UserId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	OnDelete bit NULL,
	ListRoleId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,
	AppId nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NULL
);


-- MasterData.dbo.UserMaster definition

-- Drop table

-- DROP TABLE MasterData.dbo.UserMaster;

CREATE TABLE MasterData.dbo.UserMaster (
	Id nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	UserName nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	Password nvarchar(250) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	InActive bit DEFAULT 1 NOT NULL,
	OnDelete bit DEFAULT 0 NOT NULL,
	[Role] nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,
	RoleNumber int NOT NULL,
	CONSTRAINT PK__UserMast__3214EC07C07223C3 PRIMARY KEY (Id)
);


version: '2.3'

services:
  apm-server:
    restart: on-failure
    image: docker.elastic.co/apm/apm-server:8.7.0
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
      - 8200:8200
    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.username=elastic
         -E output.elasticsearch.password=changeme
         -E output.elasticsearch.hosts=["192.168.3.162:9201"]
    mem_limit: 1g
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/
    networks:
    - elastic
  





  kibana:
    restart: on-failure
    image: docker.elastic.co/kibana/kibana:8.7.0
    environment:
      ELASTICSEARCH_HOSTS: http://192.168.3.162:9201
    #  ELASTICSEARCH_USERNAME: "elastic/kibana"
      ELASTICSEARCH_PASSWORD: "changeme"
      ELASTICSEARCH_SERVICEACCOUNTTOKEN: "AAEAAWVsYXN0aWMva2liYW5hL3Rva2VuMTo0QlhVVFAyV1R2eXUzNV9ub05UcDNn"
      server.name: kibana
      server.host: 0.0.0.0
      elasticsearch.hosts: http://192.168.3.162:9201
      monitoring.ui.container.elasticsearch.enabled: true
      ELASTICSEARCH_URL: http://192.168.3.162:9201
   #   elasticsearch.username: "elastic/kibana"
    #  elasticsearch.password: "changeme"
   #   elasticsearch.serviceAccountToken: "AAEAAWVsYXN0aWMva2liYW5hL3Rva2VuMTo0QlhVVFAyV1R2eXUzNV9ub05UcDNn"
      KIBANA_SYSTEM_PASSWORD: "abc@123"
      xpack.security.encryptionKey: "4JhT9pWqU5Oz7yXmRvF1AeB6lP8dK2cN"
    mem_limit: 1294967296
    ports:
      - 5601:5601
    networks:
      - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes: 
  elasticsearch-data:
  logstash-data:
networks:
  elastic:
    driver: bridge



















get token 
curl -X POST --user elastic:changeme   "localhost:9201/_security/service/elastic/fleet-server/credential/token/token1?pretty"
curl -X POST --user elastic:changeme   "localhost:9201/_security/service/elastic/kabana/credential/token/token1?pretty"
  kibana:
    restart: on-failure
    image: docker.elastic.co/kibana/kibana:8.7.0
    environment:
      ELASTICSEARCH_HOSTS: http://192.168.3.162:9201
    #  ELASTICSEARCH_USERNAME: "elastic/kibana"
      ELASTICSEARCH_PASSWORD: "changeme"
      ELASTICSEARCH_SERVICEACCOUNTTOKEN: "AAEAAWVsYXN0aWMva2liYW5hL3Rva2VuMTo0QlhVVFAyV1R2eXUzNV9ub05UcDNn"
      server.name: kibana
      server.host: 0.0.0.0
      elasticsearch.hosts: http://192.168.3.162:9201
      monitoring.ui.container.elasticsearch.enabled: true
      ELASTICSEARCH_URL: http://192.168.3.162:9201
   #   elasticsearch.username: "elastic/kibana"
    #  elasticsearch.password: "changeme"
   #   elasticsearch.serviceAccountToken: "AAEAAWVsYXN0aWMva2liYW5hL3Rva2VuMTo0QlhVVFAyV1R2eXUzNV9ub05UcDNn"
      KIBANA_SYSTEM_PASSWORD: "abc@123"
      xpack.security.encryptionKey: "4JhT9pWqU5Oz7yXmRvF1AeB6lP8dK2cN"
    mem_limit: 1294967296
    ports:
      - 5601:5601
    networks:
      - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status


Token Access: thêm project for shareRef

https://grid.glideapps.com/
gitlab-rails console

Feature.disable(:log_response_length)

cài

sudo apt update
sudo apt install openjdk-11-jre

Copy code
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.1-amd64.deb

sudo dpkg -i elasticsearch-7.17.1-amd64.deb

sudo service elasticsearch start
bash
Copy code
sudo systemctl enable elasticsearch





ScyllaDB
ScyllaDB’s close-to-the-metal architecture handles millions of OPS with predictable single-digit millisecond latencies.
keydb nhanh hơn redis
Refit: The automatic type-safe REST library for .NET Core, Xamarin and .NET
sqlsugar
novu

// dọn dẹp docker
Bạn đã gửi
docker buildx prune --all
Bạn đã gửi
docker builder prune --all
sudo du -h --max-depth=1 /var/lib/docker
sudo du -h --max-depth=1 /var/lib/docker | sort -rh
sudo du -h --max-depth=1 /var/lib/docker | sort -h
sudo du -ah /root | sort -h

k3s
sudo k3s crictl imagesđể xem những hình ảnh nào đã được kéo cục bộ
sudo k3s crictl rmi --pruneđể xóa bất kỳ hình ảnh nào hiện không được sử dụng bởi vùng chứa đang chạy
curl https://releases.rancher.com/install-docker/20.10.sh | sh

Theo mặc định, kubelet tích cực lưu trữ hình ảnh vào bộ nhớ đệm cho đến khi đĩa chứa kho hình ảnh đầy khoảng 85%. 
Bạn muốn điều chỉnh các giá trị --image-gc-high-thresholdvà --image-gc-low-thresholdnếu điều này không phù hợp với bạn. Xem tài liệu kubelet để biết thêm chi tiết:
https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
gitlab
12345679hopnk

sudo apt-get update
cài nginx rồi xóa trong workload traefik đi
vào phần install app sẽ show hết các thành phần liên quan


curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl


curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm


wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | TAG=v5.4.6 bash



//bo qua
kubectl -n cattle-system exec -it rancher-6757f6b675-rj42p -- /bin/bash
kubectl -n cattle-system exec -it rancher-6757f6b675-97vtb -- /bin/bash

kubectl -n cattle-system patch deployment rancher -p '{"spec": {"template": {"spec": {"containers": [{"name": "rancher", "ports": [{"containerPort": 8888, "protocol": "TCP"}]}]}}}}'
//


k3d cluster create testcluster


helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
kubectl create namespace cattle-system

# If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart:
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml
# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io
# Update your local Helm chart repository cache
helm repo update
# Install the cert-manager Helm chart
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.7.1





helm repo add rancher-stable https://releases.rancher.com/server-charts/stable


helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=rancher.my.org \
  --set bootstrapPassword=admin \
  --set replicas=2 \
  --set service.type=LoadBalancer

docker run -d --name=rancher-server --restart=unless-stopped -p 8383:8383 --privileged rancher/rancher:v2.4.18
kubectl -n cattle-system rollout status deploy/rancher
 docker logs b0766b829514 2>&1 | grep "Bootstrap Password:" ==> trả về password để vô setup
krCcESY7g3DL38Yr
 
helm upgrade rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=http://103.77.173.200/




  Node-port: sẽ call thông chủ yếu trong mạng local, có thể dùng internal ip hoặc extenal



==//
cài k3s
curl -sfL https://get.k3s.io | K3S_KUBE_VERSION=v1.26.8 sh -

move sang kubectl
 scp root@ubuntu:/etc/rancher/k3s/k3s.yaml ~/.kube/config
//
cấu hình nhiều domain trên cùng một ip của máy chủ
khi người dùng truy cập vào domain nào thì dns sẽ phân giải ip và gửi yêu cầu đó tới ip của máy chủ được cấu hình gán vào domain
trong yêu cầu đó sẽ có tên host là domain mà người dùng truy cập 
trong cấu hình ingress resource, có mục cấu hình host là gì
ingress controller sẽ dựa vào cấu hình đó so sánh với host có trong yêu cầu gửi tới và ánh xạ, gửi yêu cầu đó tới service được setup

cài dashboard hạ tầng
https://learn.netdata.cloud/docs/installing/kubernetes
chú ý chọn clusterip 19999 rồi ingress như thường


git submodule add http://192.168.3.147:8181/hacom/share/hacom.sharedrefs Sharedrefs


version: '3.8'

services:
  postgres:
    image: postgres
    restart: always
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: abc@123
      POSTGRES_DB: dbjira
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_data_back-up:/var/lib/postgresql/data-backup

  redis:
    image: redis
    restart: always
    ports:
      - "6376:6379"
    command: ["redis-server", "--requirepass", "abc@123"]
    volumes:
      - redis_data:/data
      - redis_data_back-up:/data-backup

  pgadmin:
    image: dpage/pgadmin4
    restart: always
    ports:
      - "8383:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
    depends_on:
      - postgres

  redisinsight:
    image: redislabs/redisinsight
    restart: always
    ports:
      - "8301:8001"
    depends_on:
      - redis

volumes:
  postgres_data:
  postgres_data_back-up:
  redis_data:
  redis_data_back-up:

==> ui cảu redis, nếu không có user thì có thể bỏ qua
version: '3'
services:
  rancher:
    image: rancher/rancher:v2.7.9
    ports:
      - "8333:80"
      - "8443:443"
    privileged: true
    volumes:
      - /opt/rancher:/var/lib/rancher
    environment:
      - CATTLE_SYSTEM_CATALOG=bundled
      #- SSL_CERT_PATH=/etc/rancher/ssl/myserver.crt
     # - SSL_KEY_PATH=/etc/rancher/ssl/myserver.key

==>jira
version: '3'

services:
  jira:
    image: teamatldocker/jira
    ports:
      - "8083:8080"
    volumes:
      - /path/to/jira/data:/var/atlassian/application-data/jira
    environment:
      - JVM_MINIMUM_MEMORY=1024m
      - JVM_MAXIMUM_MEMORY=2048m
      - ATL_PROXY_NAME=
      - ATL_PROXY_PORT=
      - ATL_PROXY_SCHEME=
      - ATL_TOMCAT_PORT=8083
      - DATABASE_URL=jdbc:postgresql://192.168.3.96:5432/dbjira
      - DATABASE_DRIVER=org.postgresql.Driver
      - DATABASE_USER=postgres
      - DATABASE_PASSWORD=abc@123
      - ATL_DB_TYPE=postgres
      - ATL_JDBC_URL=jdbc:postgresql://192.168.3.96:5432/dbjira
    networks:
      - jiranet


networks:
  jiranet:
    driver: bridge
==>tool ngon c2
version: '3.6'

services:
  jira:
    image: teamatldocker/jira
    ports:
      - "8083:8080"
    volumes:
      - /var/volumn/jira/data:/var/atlassian/application-data/jira
    environment:
      - JVM_MINIMUM_MEMORY=1024m
      - JVM_MAXIMUM_MEMORY=2048m
      - ATL_PROXY_NAME=
      - ATL_PROXY_PORT=
      - ATL_PROXY_SCHEME=
      - ATL_TOMCAT_PORT=8083
      - DATABASE_URL=jdbc:postgresql://192.168.3.162:5432/dbjira
      - DATABASE_DRIVER=org.postgresql.Driver
      - DATABASE_USER=postgres
      - DATABASE_PASSWORD=abc@123
      - ATL_DB_TYPE=postgres
      - ATL_JDBC_URL=jdbc:postgresql://192.168.3.162:5432/dbjira
    networks:
      - jiranet

  gitlab:
    image: gitlab/gitlab-ce
    ports:
      - "8929:8929"
      
    hostname: 'gitlab.example.com'
    restart: always
    volumes:
      - /var/volumn/gitlab/config:/etc/gitlab
      - /var/volumn/gitlab/logs:/var/log/gitlab
      - /var/volumn/gitlab/data:/var/opt/gitlab
      - /var/volumn/gitlab/backup:/var/opt/gitlab/backups
    environment:
      GITLAB_OMNIBUS_CONFIG: |
        external_url 'http://gitlab.example.com:8929'
        gitlab_rails['gitlab_shell_ssh_port'] = 22
        postgresql['enable'] = false
        gitlab_rails['db_adapter'] = "postgresql"
        gitlab_rails['db_encoding'] = "unicode"
        gitlab_rails['db_host'] = "192.168.3.162"  # Thay đổi địa chỉ IP cụ thể của PostgreSQL của bạn
        gitlab_rails['db_port'] = "5432"          # Thay đổi cổng PostgreSQL của bạn nếu cần
        gitlab_rails['db_database'] = "db-gitlab"    # Thay đổi tên cơ sở dữ liệu của bạn
        gitlab_rails['db_username'] = "postgres"  # Thay đổi tên người dùng PostgreSQL của bạn
        gitlab_rails['db_password'] = "abc@123"   # Thay đổi mật khẩu PostgreSQL của bạn
    networks:
      - jiranet

networks:
  jiranet:
    driver: bridge

cài git runner bằng lệnh apt...
vào setting ci/cd  runner new project... và làm theo hướng dẫn, chú ý cài đặt tags trong runner phải trùng với trong gitlab.yml

nano /home/gitlab-runner/.bash_logout  commen hoặc xóa file nếu lỗi chạy shell


==> để call http
{
  "insecure-registries": ["gitlab.mycompany.com:4567"]
}
/etc/docker/daemon.json
sudo systemctl status docker
sudo systemctl start docker
sudo systemctl restart docker

sudo lsof -i :8301
kill -9 1090569

05-12-2023
==>db

version: '3.8'

services:
  postgres:
    image: postgres
    restart: always
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: abc@123
      POSTGRES_DB: dbjira
      NODE_PG_FORCE_NATIVE: 1
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_data_back-up:/var/lib/postgresql/data-backup

  redis:
    image: redis
    restart: always
    ports:
      - "6379:6379"
    command: ["redis-server", "--requirepass", "abc@123"]
    volumes:
      - redis_data:/data
      - redis_data_back-up:/data-backup

  pgadmin:
    image: dpage/pgadmin4
    restart: always
    ports:
      - "8383:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@gmail.com
      PGADMIN_DEFAULT_PASSWORD: admin
    depends_on:
      - postgres

  redisinsight:
    image: redislabs/redisinsight
    restart: always
    ports:
      - "8384:8001"
    depends_on:
      - redis
  sql-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      SA_PASSWORD: "abc@1234567890"
      ACCEPT_EULA: "Y"
    ports:
      - "1433:1433"
    user: root
    volumes:
      - sql-data:/var/opt/mssql/data

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.5
    volumes:
     - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx256m -Xms256m"
      cluster.name: "docker-cluster"
      network.host: 0.0.0.0
      discovery.zen.minimum_master_nodes: 1
      discovery.type: single-node

volumes:
  postgres_data:
  sql-data:
  elasticsearch-data:
  postgres_data_back-up:
  redis_data:
  redis_data_back-up:

==>
/////////////////////elk

version: '2.2'
services:
  apm-server:
    image: docker.elastic.co/apm/apm-server:7.17.15
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
    - 8200:8200
    networks:
    - elastic
    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=["elasticsearch:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.15
    environment:
    - bootstrap.memory_lock=true
    - cluster.name=docker-cluster
    - cluster.routing.allocation.disk.threshold_enabled=false
    - discovery.type=single-node
    - ES_JAVA_OPTS=-XX:UseAVX=2 -Xms1g -Xmx1g
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
    - esdata:/usr/share/elasticsearch/data
    ports:
    - 9200:9200
    networks:
    - elastic
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.15
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
    - 5601:5601
    networks:
    - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes:
  esdata:
    driver: local

networks:
  elastic:
    driver: bridge

/////////


version: '3.8'

services:
  postgres:
    image: postgres
    restart: always
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: abc@123
      POSTGRES_DB: dbjira
      NODE_PG_FORCE_NATIVE: 1
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_data_back-up:/var/lib/postgresql/data-backup

  redis:
    image: redis
    restart: always
    ports:
      - "6379:6379"
    command: ["redis-server", "--requirepass", "abc@123"]
    volumes:
      - redis_data:/data
      - redis_data_back-up:/data-backup

  pgadmin:
    image: dpage/pgadmin4
    restart: always
    ports:
      - "8383:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@gmail.com
      PGADMIN_DEFAULT_PASSWORD: admin
    depends_on:
      - postgres

  redisinsight:
    image: redislabs/redisinsight
    restart: always
    ports:
      - "8384:8001"
    depends_on:
      - redis
  sql-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      SA_PASSWORD: "abc@1234567890"
      ACCEPT_EULA: "Y"
    ports:
      - "1433:1433"
    user: root
    volumes:
      - sql-data:/var/opt/mssql/data

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.5
    volumes:
     - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx256m -Xms256m"
      cluster.name: "docker-cluster"
      network.host: 0.0.0.0
      discovery.zen.minimum_master_nodes: 1
      discovery.type: single-node
      cluster.routing.allocation.disk.threshold_enabled: false
    ulimits:
      memlock:
        hard: -1
        soft: -1
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'
    networks:
    - elastic
    
  apm-server:
    image: docker.elastic.co/apm/apm-server:7.17.15
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
    - 8200:8200

    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=["elasticsearch:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/
    networks:
    - elastic

    
  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.15
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
    - 5601:5601
    networks:
    - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes:
  postgres_data:
  sql-data:
  elasticsearch-data:
  postgres_data_back-up:
  redis_data:
  redis_data_back-up:
networks:
  elastic:
    driver: bridge
///////////////////////////
version: '3.8'

services:
  postgres:
    image: postgres
    restart: always
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: abc@123
      POSTGRES_DB: dbjira
      NODE_PG_FORCE_NATIVE: 1
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_data_back-up:/var/lib/postgresql/data-backup

  redis:
    image: redis
    restart: always
    ports:
      - "6379:6379"
    command: ["redis-server", "--requirepass", "abc@123"]
    volumes:
      - redis_data:/data
      - redis_data_back-up:/data-backup

  pgadmin:
    image: dpage/pgadmin4
    restart: always
    ports:
      - "8383:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@gmail.com
      PGADMIN_DEFAULT_PASSWORD: admin
    depends_on:
      - postgres

  redisinsight:
    image: redislabs/redisinsight
    restart: always
    ports:
      - "8384:8001"
    depends_on:
      - redis
  sql-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      SA_PASSWORD: "abc@1234567890"
      ACCEPT_EULA: "Y"
    ports:
      - "1433:1433"
    user: root
    volumes:
      - sql-data:/var/opt/mssql/data

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.5
    volumes:
     - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx525m -Xms256m"
      cluster.name: "docker-cluster"
      network.host: 0.0.0.0
      discovery.zen.minimum_master_nodes: 1
      discovery.type: single-node
      cluster.routing.allocation.disk.threshold_enabled: false
   # ulimits:
     # memlock:
      #  hard: -1
     #   soft: -1
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'
    networks:
    - elastic
    
  apm-server:
    restart: always
    image: docker.elastic.co/apm/apm-server:7.17.15
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
    - 8200:8200

    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=["elasticsearch:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/
    networks:
    - elastic
  
  kibana:
    restart: always
    image: docker.elastic.co/kibana/kibana:7.17.15
    depends_on:
      elasticsearch:
       condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
    - 5601:5601
    networks:
    - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

  logstash:
    container_name: logstash
    restart: always
    image: docker.elastic.co/logstash/logstash:7.17.5
    depends_on:
      elasticsearch:
       condition: service_healthy
    volumes:
      - logstash-data:/usr/share/logstash
    command: >
       logstash-plugin install logstash-input-http
    ports:
      - "8686:8080"
      - 5044:5044
      - 9600:9600
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"     


volumes:
  postgres_data:
  sql-data:
  elasticsearch-data:
  postgres_data_back-up:
  redis_data:
  redis_data_back-up:
  logstash-data:
networks:
  elastic:
    driver: bridge
//////////////elk

version: '3.8'

services:
  elasticsearch:
    container_name: elasticsearch
    restart: always
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.5
    volumes:
      - es_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - elk
    environment:
      ES_JAVA_OPTS: "-Xmx512m -Xms256m"
      discovery.type: single-node
      cluster.name: "docker-cluster"
      network.host: 0.0.0.0
      discovery.zen.minimum_master_nodes: 1
      #ELASTIC_PASSWORD: abc@123

  logstash:
    container_name: logstash
    restart: unless-stopped
    image: docker.elastic.co/logstash/logstash:7.17.5
    
    command: >
      sh -c '
        logstash-plugin install logstash-input-http
      '
    volumes:
      - ./elk/logstash/config:/usr/share/logstash/config
      - ./elk/logstash/pipeline:/usr/share/logstash/pipeline
      - ./elk/logstash/data:/usr/share/logstash/data
      - ./elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
    ports:
      - "5044:5044"

      - "8080:8080"
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms256m"
   #   network.host: 0.0.0.0
    #  path.config: "/usr/share/logstash/pipeline"
    networks:
      - elk
    depends_on:
      - elasticsearch

volumes:
  es_data:
networks:
  elk:
    driver: bridge
///////////////////

version: '3.8'

services:

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.5
    volumes:
     - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx525m -Xms256m"
      cluster.name: "docker-cluster"
      network.host: 0.0.0.0
      discovery.zen.minimum_master_nodes: 1
      discovery.type: single-node
      cluster.routing.allocation.disk.threshold_enabled: false
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'
    networks:
    - elastic
      
 
  apm-server:
    restart: always
    image: docker.elastic.co/apm/apm-server:7.17.5
    depends_on:
      elasticsearch:
        condition: service_healthy
    #  kibana:
      #  condition: service_healthy
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
      - 8200:8200
    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=["elasticsearch:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/
    networks:
    - elastic
  
  
  logstash:
    container_name: logstash
    restart: always
    image: docker.elastic.co/logstash/logstash:7.17.5
    depends_on:
      elasticsearch:
       condition: service_healthy
    volumes:
     # - logstash-data:/usr/share/logstash
      - ./elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
    command: >
       logstash-plugin install logstash-input-http
    ports:
      - "8686:8080"
      - "5045:5044"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xms512m -Xmx512m"
    networks:
     - elastic
       
  kibana:
    restart: always
    image: docker.elastic.co/kibana/kibana:7.17.5
    depends_on:
      elasticsearch:
         condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - 5601:5601
    networks:
      - elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes: 
  elasticsearch-data:
  logstash-data:
networks:
  elastic:
    driver: bridge

///////////
docker exec -it bf5620aa2c71 logstash-plugin install logstash-input-http

/// oracle
docker login container-registry.oracle.com
hopxc1997@gmail.com    
Aa
docker run -d --name oracle-db \
  -e DB_SID=ORCLCDB \
  -e DB_PDB=ORCLPDB1 \
  -e DB_CHARACTERSET=AL32UTF8 \
  -e DB_MEMORY=5GB \
  -e ORACLE_PWD=abc@123Hac0m \
  -e NLS_LANG=VIETNAMESE_VIETNAM.AL32UTF8 \
  -p 1521:1521 \
  -p 5500:5500 \
  --network oracle-net \
  -v $(pwd)/data:/opt/oracle/oradata \
  -v data-oracle:/opt/oracle/oradata \
  container-registry.oracle.com/database/enterprise:19.3.0.0


docker exec -it oracle-db sqlplus -L sys/abc@123Hac0m//localhost:1521/ORCLCDB as sysdba @healthcheck.sql

